{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac6d105-0a2f-4830-a9d8-b2ba6bfddc16",
   "metadata": {},
   "source": [
    "# Optical Character Detection using TAO OCNet\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee889e-4166-4954-98dc-f36320ead6de",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained OCDNet model and train OCDNet model on the IAMDATA Handwritting dataset\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of OCDNet using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Installing the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2) <br>\n",
    "    2.1 [Download pre-trained model](#head-2-2) <br>\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate](#head-5)\n",
    "6. [Inference](#head-6)\n",
    "7. [Deploy](#head-7)\n",
    "TAO training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ecd1c-7b50-422e-9701-d13df6730850",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9535e741-3c3c-4f54-8806-2f0a02bea98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOCAL_PROJECT_DIR=/home/pdelafuente/tao/my_apps/ocdnet\n",
      "env: NOTEBOOK_DIR=/home/pdelafuente/tao/my_apps/ocdnet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "%env LOCAL_PROJECT_DIR=/home/pdelafuente/tao/my_apps/ocdnet\n",
    "%env NOTEBOOK_DIR=/home/pdelafuente/tao/my_apps/ocdnet\n",
    "os.environ[\"HOST_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"ocdnet\")\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"ocdnet\", \"results\")\n",
    "os.environ[\"PRE_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"iamdata\")\n",
    "\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/ocdnet\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_DIR\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e2d72-1e1a-401c-87c9-ec4cd108d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR\n",
    "! mkdir -p $PRE_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff606239-5e90-49a0-b377-46f35bf7e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tlt_configs = {\n",
    "   \"Mounts\":[\n",
    "       # Mapping the data directory\n",
    "       {\n",
    "           \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "           \"destination\": \"/workspace/tao/my_apps/ocdnet\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/workspace/tao/my_apps/ocdnet/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/workspace/tao/my_apps/ocdnet/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/workspace/tao/my_apps/ocdnet/results\"\n",
    "       },\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tlt_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b85094fe-83ab-4cc1-bf11-3d65cfd9099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Mounts\": [\n",
      "        {\n",
      "            \"source\": \"/home/pdelafuente/tao/my_apps/ocdnet\",\n",
      "            \"destination\": \"/workspace/tao/my_apps/ocdnet\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/pdelafuente/tao/my_apps/ocdnet/data/ocdnet\",\n",
      "            \"destination\": \"/workspace/tao/my_apps/ocdnet/data\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/pdelafuente/tao/my_apps/ocdnet/specs\",\n",
      "            \"destination\": \"/workspace/tao/my_apps/ocdnet/specs\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/pdelafuente/tao/my_apps/ocdnet/ocdnet/results\",\n",
      "            \"destination\": \"/workspace/tao/my_apps/ocdnet/results\"\n",
      "        }\n",
      "    ],\n",
      "    \"DockerOptions\": {\n",
      "        \"shm_size\": \"16G\",\n",
      "        \"ulimits\": {\n",
      "            \"memlock\": -1,\n",
      "            \"stack\": 67108864\n",
      "        }\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3c415-0f51-4cc8-a5b0-9ddb21971536",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in PyPI. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.6.9 < 3.8.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838ef24-0ff0-4c83-815d-62cf478e619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher or you plan to run directly from the container\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86511850-311e-4563-a077-40ebf29661b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\n",
      "task_group: ['model', 'dataset', 'deploy']\n",
      "format_version: 3.0\n",
      "toolkit_version: 5.0.0\n",
      "published_date: 05/08/2023\n"
     ]
    }
   ],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114a774-54f0-4608-9987-4a1278e788ac",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9dae7-8caf-41d8-898f-adeb04b9bbd2",
   "metadata": {},
   "source": [
    "We will be using the IAM handwritting dataset. To find more details please visit\n",
    "https://fki.tic.heia-fr.ch/databases/iam-handwriting-database. You will need need to register for a free account and manually downlaod the required files. Please review the licensing requirements. These files are to be used for research use only.  \n",
    "\n",
    "There are several zipped files to download, the ascii.tgz and the document images which are each stored in one of 3 zipped files based on the document title.\n",
    "Place these files in your data/iamdata folder ($PRE_DATA_DIR)\n",
    "ascii.tgz\n",
    "formsA-D.tgz\n",
    "formsE-H.tgz\n",
    "formsI-Z.tgz\n",
    "\n",
    "Please download the IAMDATA (https://rrc.cvc.uab.es/?ch=4&com=downloads) to `$PRE_DATA_DIR`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ecab0f6-a489-4692-b934-0e444ce3f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "forms.txt\n",
      "lines.txt\n",
      "words.txt\n",
      "sentences.txt\n"
     ]
    }
   ],
   "source": [
    "#the ascii tar file has several files of metadata for the iam dataset as. Run the command in this block to see the list of files. \n",
    "#We just need the words.txt file which contains the words detected and  x,y,h,w info for each for each image\n",
    "!tar -tf $PRE_DATA_DIR/ascii.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43262c5e-f03c-4871-8129-6618e9855785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets extract just the words.txt file into our data/iamdata directory\n",
    "#The words.txt file contains the words detected and  x,y,h,w info for each for each image\n",
    "!tar -xf $PRE_DATA_DIR/ascii.tgz --directory $PRE_DATA_DIR/ words.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a009e3e-172f-425f-87ed-43b876d05b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529\n",
      "522\n",
      "488\n"
     ]
    }
   ],
   "source": [
    "#let's see how many images are in each file. \n",
    "!tar -tzf $PRE_DATA_DIR/formsA-D.tgz | wc -l\n",
    "!tar -tzf $PRE_DATA_DIR/formsE-H.tgz | wc -l\n",
    "!tar -tzf $PRE_DATA_DIR/formsI-Z.tgz | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc596087-5aed-4e84-b981-93fb1e7a0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories to hold image data\n",
    "!mkdir -p $PRE_DATA_DIR/train/img\n",
    "!mkdir -p $PRE_DATA_DIR/test/img\n",
    "!mkdir -p $PRE_DATA_DIR/train/gt\n",
    "!mkdir -p $PRE_DATA_DIR/test/gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba892897-d763-4e74-a2d6-424e255c523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack the images, let's use the first two groups of images for training and the last for validation\n",
    "\n",
    "!tar -xzf $PRE_DATA_DIR/formsA-D.tgz --directory $PRE_DATA_DIR/train/img\n",
    "!tar -xzf $PRE_DATA_DIR/formsE-H.tgz --directory $PRE_DATA_DIR/train/img \n",
    "!tar -xzf $PRE_DATA_DIR/formsI-Z.tgz --directory $PRE_DATA_DIR/test/img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "618d4d0c-5fea-45bf-b61a-461bee23850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxrwxr-x 2 pdelafuente pdelafuente  4096 Jul  3 12:57 gt\n",
      "drwxrwxr-x 2 pdelafuente pdelafuente 20480 Jul  3 12:59 img\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "!ls -l $PRE_DATA_DIR/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c043ab4a-c040-47a1-884e-806b4ce2ac25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gt_a01-000u.txt</td>\n",
       "      <td>507</td>\n",
       "      <td>766</td>\n",
       "      <td>720</td>\n",
       "      <td>766</td>\n",
       "      <td>720</td>\n",
       "      <td>814</td>\n",
       "      <td>507</td>\n",
       "      <td>814</td>\n",
       "      <td>MOVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gt_a01-000u.txt</td>\n",
       "      <td>796</td>\n",
       "      <td>764</td>\n",
       "      <td>866</td>\n",
       "      <td>764</td>\n",
       "      <td>866</td>\n",
       "      <td>814</td>\n",
       "      <td>796</td>\n",
       "      <td>814</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gt_a01-000u.txt</td>\n",
       "      <td>919</td>\n",
       "      <td>757</td>\n",
       "      <td>1085</td>\n",
       "      <td>757</td>\n",
       "      <td>1085</td>\n",
       "      <td>835</td>\n",
       "      <td>919</td>\n",
       "      <td>835</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gt_a01-000u.txt</td>\n",
       "      <td>1185</td>\n",
       "      <td>754</td>\n",
       "      <td>1311</td>\n",
       "      <td>754</td>\n",
       "      <td>1311</td>\n",
       "      <td>815</td>\n",
       "      <td>1185</td>\n",
       "      <td>815</td>\n",
       "      <td>Mr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gt_a01-000u.txt</td>\n",
       "      <td>1438</td>\n",
       "      <td>746</td>\n",
       "      <td>1820</td>\n",
       "      <td>746</td>\n",
       "      <td>1820</td>\n",
       "      <td>819</td>\n",
       "      <td>1438</td>\n",
       "      <td>819</td>\n",
       "      <td>Gaitskell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename     x    y    x2   y2    x3   y3    x4   y4       word\n",
       "0  gt_a01-000u.txt   507  766   720  766   720  814   507  814       MOVE\n",
       "1  gt_a01-000u.txt   796  764   866  764   866  814   796  814         to\n",
       "2  gt_a01-000u.txt   919  757  1085  757  1085  835   919  835       stop\n",
       "3  gt_a01-000u.txt  1185  754  1311  754  1311  815  1185  815        Mr.\n",
       "4  gt_a01-000u.txt  1438  746  1820  746  1820  819  1438  819  Gaitskell"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_columns(line):\n",
    "    #filename = line[:line.index('-') + 5]\n",
    "    parts = line.split()\n",
    "    file = parts[0]\n",
    "    if len(file) == 13:\n",
    "        filename = 'gt_' + line[:line.index('-') + 4] + '.txt'\n",
    "    else:\n",
    "        filename = 'gt_' + line[:line.index('-') + 5] + '.txt'\n",
    "    \n",
    "    word = parts[-1]\n",
    "    x = parts[3]\n",
    "    y = parts[4]\n",
    "    w = parts[5]\n",
    "    h = parts[6]\n",
    "    x2 = int(x) + int(w)\n",
    "    y2 = y\n",
    "    x3 = int(x) + int(w)\n",
    "    y3 = int(y) + int(h)\n",
    "    x4 = x\n",
    "    y4 = int(y) + int(h)\n",
    "    return filename,  x, y, x2, y2, x3, y3, x4, y4, word\n",
    "\n",
    "def process_text_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        #skip the first 18 lines in the words.txt file as that is just meta data \n",
    "        lines = file.readlines()[19:] \n",
    "        for line in lines:\n",
    "            filename, x, y, x2, y2, x3, y3, x4, y4,word = extract_columns(line.strip()) \n",
    "            if not word == ', ,' and not word == '. .' or not word == ''and not word == '*- -' and not word == '; ;' and not word == '.' and not word == ',':\n",
    "                data.append([filename, x, y, x2, y2, x3, y3, x4, y4, word])\n",
    "    # Create a DataFrame from the extracted data\n",
    "    columns = ['filename', 'x', 'y','x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'word' ] \n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "pfile = os.environ[\"PRE_DATA_DIR\"] + '/words.txt'\n",
    "df = process_text_file(pfile)\n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5197be-104a-48d6-a340-aaccc0c71c1e",
   "metadata": {},
   "source": [
    "This command will group the dataframe from the previous step and create a txt file for each image with the 8 point coordinates and the identified words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81304585-13d5-4b4f-9c75-81a9b5b68d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write out the label files for each image and save to the applicable test or training gt folder\n",
    "groups = df.groupby('filename')\n",
    "for filename, group in groups:\n",
    "    gdf = pd.DataFrame(data=group)\n",
    "    pdf = gdf[['x', 'y', 'x2', 'y2','x3','y3','x4','y4','word']]\n",
    "    if filename[3] in ['j','k','l','m','n','p','r']:\n",
    "        tefile = os.environ[\"PRE_DATA_DIR\"] + '/test/gt/' + filename\n",
    "        #pdf.to_csv(f'data/iamdata\\test\\gt\\{filename}', index=False, header=False)\n",
    "        pdf.to_csv(tefile, index=False, header=False)\n",
    "    else:\n",
    "        trfile = os.environ[\"PRE_DATA_DIR\"] + '/train/gt/' + filename\n",
    "        pdf.to_csv(trfile, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4147edbe-f120-4b82-83db-4e84f213e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets copy our data from the data prep directory to the host data directory\n",
    "#you may wish to remove the original zip files and images in the data prep directory once you copy the data to the host data directory\n",
    "!cp -a $PRE_DATA_DIR/train/. $HOST_DATA_DIR/train/\n",
    "!cp -a $PRE_DATA_DIR/test/. $HOST_DATA_DIR/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e8ea5-8e80-4cc0-a875-f25f8bfc331d",
   "metadata": {},
   "source": [
    "Next we download a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a8e6b-f294-479d-b464-f2a886730af1",
   "metadata": {},
   "source": [
    "## 2.2 Prepare Pretrained Models <a class=\"anchor\" id=\"head-2-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bac55-4223-4f4d-b4e9-39eed5b204cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's download a pretrained model\n",
    "!pip install gdown\n",
    "#Download deformable_resnet18 pretrained model\n",
    "!gdown https://drive.google.com/uc?id=16U4Smk6k3cFxxU8NhXC-VQmMQ220VgYH -O $HOST_DATA_DIR/ocdnet_deformable_resnet18.pth\n",
    "#Download deformable_resnet50 pretrained model\n",
    "!gdown https://drive.google.com/uc?id=1qkv6pDYopwlrLb9uAtI8n0gyVX0kn3GQ -O $HOST_DATA_DIR/ocdnet_deformable_resnet50.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c3e4e-d44c-41f3-846e-5ccba88e49f8",
   "metadata": {},
   "source": [
    "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "We provide specification files to configure the training parameters including:\n",
    "\n",
    "* num_gpus: number of gpus \n",
    "* train: configure the training hyperparameters\n",
    "    * results_dir: Path to restore training result\n",
    "    * resume_training_checkpoint_path: Resume training from a checkpoint.\n",
    "    * num_epochs: The total epochs for training\n",
    "    * validation_interval: validation interval\n",
    "    * checkpoint_interval: checkpoint interval\n",
    "    * optimizer\n",
    "        * type: Defaults to Adam.\n",
    "        * lr: Initial learning rate \n",
    "    * lr_scheduler\n",
    "        * type: Only supports WarmupPolyLR\n",
    "        * warmup_epoch: The epoch numbers for warm up to initinal learning rate. It should be different from num_epochs. \n",
    "    * post_processing\n",
    "        * type: Only supports SegDetectorRepresenter\n",
    "        * thresh: The threshold for binarization.\n",
    "        * box_thresh: The threshold for bounding box.\n",
    "        * unclip_ratio: Default to 1.5. The box will look larger if this ratio is set to larger.\n",
    "    * Metric\n",
    "        * type: Only supports QuadMetric\n",
    "        * is_output_polygon: Defaults to false. False for bounding box. True for polygon.\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * train_dataset:\n",
    "        * data_path: Path to train images. If there are multi sources, set it looks like ['/path/1' , '/path/2']\n",
    "        * pre_processes\n",
    "            * size: Ramdom crop size during training. Defaults to [640, 640].\n",
    "        * loader\n",
    "            * batch_size: batch size for dataloader\n",
    "            * num_workers: number of workers to do data loading \n",
    "    * validate_dataset: \n",
    "        * data_path: Path to validation images. If there are multi sources, set it looks like ['/path/1' , '/path/2']\n",
    "        * pre_processes\n",
    "            * short_size: Resize to width x height during evaluation. Defaults to [1280, 736].\n",
    "            * resize_text_polys: Resize the coordinate of text groudtruth. Defaults to true.\n",
    "        * loader\n",
    "            * batch_size: batch size for dataloader\n",
    "            * num_workers: number of workers to do data loading            \n",
    "* model: configure the model setting\n",
    "    * backbone: The backbone type. The deformable_resnet18 and deformable_resnet50 are supported.\n",
    "    * load_pruned_graph: Defaults to False. Must set to true if train a model which is pruned. \n",
    "    * pruned_graph_path: The path to the pruned model graph.\n",
    "    * pretrained_model_path: Finetune from a pretrained model. The `.pth` model is supported.\n",
    "\n",
    "Please refer to the TAO documentation about OCDNet to get more parameters that are configurable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eed6d1-3f2b-4e83-a9fc-5ced15f1a65f",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "Now we train the pretrained model in OCDNET with the IAM dataset. \n",
    "\n",
    "If you opt to just run from within the container so this needs to be run on a machine with a GPU, Docker and NVIDIA drivers.\n",
    "These are the two TAO containers we are using in this notebook:\n",
    "1. nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-pyt\n",
    "2. nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-deploy\n",
    "\n",
    "Container #1 is used to train, evaluate,  and inference TAO models.\n",
    "Container #2 is used to export models and generate tensor rt engines for deployment. \n",
    "\n",
    "Doublecheck the files in the specs directory that are used as parameters in the command. Below we are using the train.yaml and evaluate.yaml.  Make sure the directories in those files that point to the results and datasets are accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57b0d6-577d-486f-bc91-155db86f252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR=/workspace/tao/my_apps/ocdnet/data/ocdnet\n",
    "%env SPECS_DIR=/workspace/tao/my_apps/ocdnet/specs\n",
    "%env RESULTS_DIR=/workspace/tao/my_apps/ocdnet/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99062688-72cc-480f-8d04-13317bae76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train using TAO Launcher\n",
    "#print(\"Run training with ngc pretrained model.\")\n",
    "!tao model ocdnet train \\\n",
    "          -e $SPECS_DIR/train.yaml \\\n",
    "          -r $RESULTS_DIR/train -k $KEY \\\n",
    "          model.pretrained_model_path=$DATA_DIR/ocdnet_deformable_resnet18.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced02cb5-542d-4b73-b6ff-19847cf98027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI only if using direct container calls, do not both the previous and this block, just pick one option \n",
    "#Train using a direct call to docker container\n",
    "\n",
    "#docker command to train model with labeled dataset\n",
    "#better to run from terminal, make sure to mount project directory\n",
    "!docker run -it --rm --gpus all nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-pyt \\\n",
    "ocdnet train -e $SPECS_DIR/train.yaml -r $RESULTS_DIR/train -k $KEY \\\n",
    "model.pretrained_model_path=$DATA_DIR/ocdnet_deformable_resnet18.pth -v ~/home/pdelafuente/tao:/workspace/tao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9754b7-d92f-4cc6-aeae-c532fc696d5f",
   "metadata": {},
   "source": [
    "## 5. Evaluate a trained model <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "In this section, we run the `evaluate` tool to evaluate the trained model and produce the evaluation metric.\n",
    "\n",
    "We provide specification files to configure the parameters including:\n",
    "\n",
    "* evaluate: configure the training hyperparameters\n",
    "    * results_dir: Path to restore training result\n",
    "    * checkpoint: checkpoint path for running evaluation\n",
    "    * post_processing\n",
    "        * type: Only supports SegDetectorRepresenter\n",
    "        * thresh: The threshold for binarization.\n",
    "        * box_thresh: The threshold for bounding box.\n",
    "        * unclip_ratio: Default to 1.5. The box will look larger if this ratio is set to larger.\n",
    "    * Metric\n",
    "        * type: Only supports QuadMetric\n",
    "        * is_output_polygon: Defaults to false. False for bounding box. True for polygon.\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * validate_dataset: \n",
    "        * data_path: Path to validation images. If there are multi sources, set it looks like ['/path/1' , '/path/2']\n",
    "        * pre_processes\n",
    "            * short_size: Resize to width x height during evaluation. Defaults to [1280, 736].\n",
    "            * resize_text_polys: Resize the coordinate of text groudtruth. Defaults to true.\n",
    "        * loader\n",
    "            * batch_size: batch size for dataloader\n",
    "            * num_workers: number of workers to do data loading            \n",
    "* model: configure the model setting\n",
    "    * backbone: The backbone type. The deformable_resnet18 and deformable_resnet50 are supported.\n",
    "    * load_pruned_graph: Whether evaluation a model which has pruned model graph. Defaults to False.\n",
    "    * pruned_graph_path: The path to the pruned model graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e98a1-82c6-4312-ae05-7f73012fde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on model\n",
    "!tao model ocdnet evaluate \\\n",
    "            -e $SPECS_DIR/evaluate.yaml \\\n",
    "            evaluate.checkpoint=$RESULTS_DIR/train/model_best.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4eca20-796b-4a90-9ab1-58630d947b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI only if using direct container calls, do not both the previous and this block, just pick one option \n",
    "#evaluate trained model\n",
    "!docker run -it --rm --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/pdelafuente/tao:/workspace/tao --gpus all nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-pyt \\\n",
    "ocdnet evaluate -e $SPECS_DIR/evaluate.yaml \\\n",
    "evaluate.checkpoint=$RESULTS_DIR/train/model_best.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd8a92-641d-4271-b61c-9a11262d1c9d",
   "metadata": {},
   "source": [
    "## 6. Visualize Inferences <a class=\"anchor\" id=\"head-6\"></a>\n",
    "In this section, we run the `inference` tool to generate inferences on the trained models and visualize the results. The `inference` tool produces annotated image outputs and txt files that contain prediction information.\n",
    "\n",
    "We provide specification files to configure the inference parameters including:\n",
    "\n",
    "* inference: configure the training hyperparameters\n",
    "    * results_dir: Path to restore inference result\n",
    "    * checkpoint: checkpoint path for running inference\n",
    "    * input_folder: The input folder for inference\n",
    "    * width: The width for resizing\n",
    "    * height: The height for resizing\n",
    "    * polygon: Produce polygon(true) or bounding box(false). Defaults to false.\n",
    "    * post_processing\n",
    "        * type: Only supports SegDetectorRepresenter\n",
    "        * thresh: The threshold for binarization.\n",
    "        * box_thresh: The threshold for bounding box.\n",
    "        * unclip_ratio: Default to 1.5. The box will look larger if this ratio is set to larger.       \n",
    "* model: configure the model setting\n",
    "    * backbone: The backbone type. The deformable_resnet18 and deformable_resnet50 are supported.\n",
    "    * load_pruned_graph: Whether evaluation a model which has pruned model graph. Defaults to False.\n",
    "    * pruned_graph_path: The path to the pruned model graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f13014-dc4d-4070-b3d9-a0b4e4612632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run inference using TAO\n",
    "!tao model ocdnet inference \\\n",
    "           -e $SPECS_DIR/inference.yaml \\\n",
    "           inference.checkpoint=$RESULTS_DIR/train/model_best.pth \\\n",
    "           inference.input_folder=$DATA_DIR/test/img \\\n",
    "           inference.results_dir=$RESULTS_DIR/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0c557-e121-4ff1-af20-5afddb2e541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI only if using direct container calls, do not both the previous and this block, just pick one option  \n",
    "#run inference using direct TAO container call\n",
    "!docker run -it --rm --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/pdelafuente/tao:/workspace/tao --gpus all nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-pyt \\\n",
    "ocdnet inference -e /workspace/tao/my_apps/ocdnet/specs/inference.yaml \\\n",
    "inference.checkpoint=$RESULTS_DIR/train/model_best.pth \\\n",
    "inference.input_folder=$DATA_DIR/test/img \\\n",
    "inference.results_dir=$RESULTS_DIR/inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967bfd3b-35e1-42d9-9d51-f71706f1aee8",
   "metadata": {},
   "source": [
    "## 7. Deploy <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b5b85-c12e-4779-ac31-a067eb8fe85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export pth model to ONNX model\n",
    "!tao model ocdnet export \\\n",
    "           -e $SPECS_DIR/export.yaml \\\n",
    "           export.checkpoint=$RESULTS_DIR/train/model_best.pth \\\n",
    "           export.onnx_file=$RESULTS_DIR/export/model_best.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce50283-f75c-44ed-8dcd-f5431cb29afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "=== TAO Toolkit PyTorch ===\n",
      "===========================\n",
      "\n",
      "NVIDIA Release 4.0.0-PyTorch (build 53420872)\n",
      "TAO Toolkit Version 4.0.0\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the TAO Toolkit End User License Agreement.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/tao-toolkit-software-license-agreement\n",
      "\n",
      "[2023-06-27 20:36:41,152 - TAO Toolkit - torch.distributed.nn.jit.instantiator - INFO] Created a temporary directory at /tmp/tmp56q0zajx\n",
      "[2023-06-27 20:36:41,152 - TAO Toolkit - torch.distributed.nn.jit.instantiator - INFO] Writing /tmp/tmp56q0zajx/_remote_module_non_scriptable.py\n",
      "[2023-06-27 20:36:41,613 - TAO Toolkit - matplotlib.font_manager - INFO] generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'export.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "<frozen core.hydra.hydra_runner>:107: UserWarning: \n",
      "'export.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "Experiment configuration:\n",
      "train:\n",
      "  results_dir: null\n",
      "  resume_training_checkpoint_path: null\n",
      "  num_epochs: 50\n",
      "  checkpoint_interval: 1\n",
      "  validation_interval: 1\n",
      "  gpu_id:\n",
      "  - 0\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: ???\n",
      "      box_thresh: ???\n",
      "      max_candidates: ???\n",
      "      unclip_ratio: ???\n",
      "  metric:\n",
      "    type: QuadMetric\n",
      "    args:\n",
      "      is_output_polygon: ???\n",
      "  trainer:\n",
      "    is_output_polygon: false\n",
      "    warmup_epoch: 3\n",
      "    seed: 2\n",
      "    log_iter: 10\n",
      "    clip_grad_norm: 5.0\n",
      "    show_images_iter: 50\n",
      "    tensorboard: false\n",
      "  loss:\n",
      "    type: DBLoss\n",
      "    alpha: 5\n",
      "    beta: 10\n",
      "    ohem_ratio: 3\n",
      "  optimizer:\n",
      "    type: Adam\n",
      "    args:\n",
      "      lr: 0.001\n",
      "      weight_decay: 0.0\n",
      "      amsgrad: true\n",
      "  lr_scheduler:\n",
      "    type: WarmupPolyLR\n",
      "    args:\n",
      "      warmup_epoch: ???\n",
      "model:\n",
      "  backbone: deformable_resnet18\n",
      "  pretrained: false\n",
      "  in_channels: 3\n",
      "  neck: FPN\n",
      "  inner_channels: 256\n",
      "  head: DBHead\n",
      "  out_channels: 2\n",
      "  k: 50\n",
      "  load_pruned_graph: false\n",
      "  pruned_graph_path: /results/prune/pruned_0.1.pth\n",
      "  pretrained_model_path: null\n",
      "evaluate:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  trt_engine: null\n",
      "  gpu_id: 0\n",
      "  batch_size: 1\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: ???\n",
      "      box_thresh: ???\n",
      "      max_candidates: ???\n",
      "      unclip_ratio: ???\n",
      "  metric:\n",
      "    type: QuadMetric\n",
      "    args:\n",
      "      is_output_polygon: ???\n",
      "dataset:\n",
      "  train_dataset:\n",
      "    data_name: ICDAR2015Dataset\n",
      "    data_path: ???\n",
      "    args:\n",
      "      img_mode: BGR\n",
      "      filter_keys:\n",
      "      - img_path\n",
      "      - img_name\n",
      "      - text_polys\n",
      "      - texts\n",
      "      - ignore_tags\n",
      "      - shape\n",
      "      ignore_tags:\n",
      "      - '*'\n",
      "      - '###'\n",
      "      pre_processes: null\n",
      "    loader:\n",
      "      batch_size: 32\n",
      "      shuffle: true\n",
      "      pin_memory: false\n",
      "      num_workers: 0\n",
      "      collate_fn: ''\n",
      "  validate_dataset:\n",
      "    data_name: ICDAR2015Dataset\n",
      "    data_path:\n",
      "    - /workspace/tao/my_apps/ocdnet/data/ocdnet/test\n",
      "    args:\n",
      "      img_mode: BGR\n",
      "      filter_keys:\n",
      "      - ''\n",
      "      ignore_tags:\n",
      "      - '*'\n",
      "      - '###'\n",
      "      pre_processes: null\n",
      "    loader:\n",
      "      batch_size: 1\n",
      "      shuffle: false\n",
      "      pin_memory: false\n",
      "      num_workers: 0\n",
      "      collate_fn: ICDARCollateFN\n",
      "export:\n",
      "  results_dir: /results/export\n",
      "  checkpoint: /workspace/tao/my_apps/ocdnet/results/train/model_best.pth\n",
      "  onnx_file: /workspace/tao/my_apps/ocdnet/results/export/model_bestocdnet.onnx\n",
      "  gpu_id: 0\n",
      "  width: 1280\n",
      "  height: 736\n",
      "  opset_version: 11\n",
      "gen_trt_engine:\n",
      "  results_dir: null\n",
      "  gpu_id: 0\n",
      "  onnx_file: ???\n",
      "  trt_engine: ???\n",
      "  width: ???\n",
      "  height: ???\n",
      "  img_mode: BGR\n",
      "  tensorrt:\n",
      "    data_type: FP32\n",
      "    workspace_size: 1024\n",
      "    min_batch_size: 1\n",
      "    opt_batch_size: 1\n",
      "    max_batch_size: 1\n",
      "    calibration:\n",
      "      cal_image_dir: ???\n",
      "      cal_cache_file: ???\n",
      "      cal_batch_size: 1\n",
      "      cal_num_batches: 1\n",
      "inference:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  trt_engine: null\n",
      "  input_folder: ???\n",
      "  width: ???\n",
      "  height: ???\n",
      "  img_mode: ???\n",
      "  polygon: true\n",
      "  show: false\n",
      "  gpu_id: 0\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: ???\n",
      "      box_thresh: ???\n",
      "      max_candidates: ???\n",
      "      unclip_ratio: ???\n",
      "prune:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  gpu_id: 0\n",
      "  batch_size: 1\n",
      "  pruning_thresh: ???\n",
      "name: ???\n",
      "num_gpus: 1\n",
      "results_dir: ???\n",
      "\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "Starting OCDNet export\n",
      "========== Diagnostic Run torch.onnx.export version 1.14.0a0+44dac51 ===========\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model exported to /workspace/tao/my_apps/ocdnet/results/export/model_bestocdnet.onnx\n",
      "Export finished successfully.\n"
     ]
    }
   ],
   "source": [
    "#FYI only if using direct container calls, do not both the previous and this block, just pick one option \n",
    "#export model to onnx format\n",
    "!docker run -it --rm --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/pdelafuente/tao:/workspace/tao --gpus all nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-pyt \\\n",
    "ocdnet export -e $SPECS_DIR/export.yaml \\\n",
    "export.checkpoint=$RESULTS_DIR/train/model_best.pth \\\n",
    "export.onnx_file=$RESULTS_DIR/export/model_best.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e826801-2831-4fb2-b1ab-8b8e339c0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TensorRT engine using tao-deploy\n",
    "!tao deploy ocdnet gen_trt_engine -e $SPECS_DIR/gen_trt_engine.yaml \\\n",
    "                               gen_trt_engine.onnx_file=$RESULTS_DIR/export/model_best.onnx \\\n",
    "                               gen_trt_engine.tensorrt.min_batch_size=1 \\\n",
    "                               gen_trt_engine.tensorrt.opt_batch_size=3 \\\n",
    "                               gen_trt_engine.tensorrt.max_batch_size=3 \\\n",
    "                               gen_trt_engine.tensorrt.data_type=int8 \\\n",
    "                               gen_trt_engine.trt_engine=$RESULTS_DIR/export/ocdnet_model.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fb02a3-31b9-4d7f-8c5a-b86c3939f3f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================\n",
      "=== TAO Toolkit Deploy ===\n",
      "=======================\n",
      "\n",
      "NVIDIA Release 4.0.0-Deploy (build 52693241)\n",
      "TAO Toolkit Version 4.0.0\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the TAO Toolkit End User License Agreement.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/tao-toolkit-software-license-agreement\n",
      "\n",
      "2023-06-27 20:49:06,836 [INFO] matplotlib.font_manager: generated new fontManager\n",
      "python /usr/local/lib/python3.8/dist-packages/nvidia_tao_deploy/cv/ocdnet/scripts/gen_trt_engine.py  --config-path /workspace/tao/my_apps/ocdnet/specs --config-name gen_trt_engine.yaml gen_trt_engine.onnx_file=/workspace/tao/my_apps/ocdnet/results/export/model_bestocdnet.onnx gen_trt_engine.tensorrt.min_batch_size=1 gen_trt_engine.tensorrt.opt_batch_size=3 gen_trt_engine.tensorrt.max_batch_size=3 gen_trt_engine.tensorrt.data_type=int8 gen_trt_engine.trt_engine=/workspace/tao/my_apps/ocdnet/results/export/ocdnet_model.engine\n",
      "sys:1: UserWarning: \n",
      "'gen_trt_engine.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "<frozen cv.common.hydra.hydra_runner>:86: UserWarning: \n",
      "'gen_trt_engine.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "Starting ocdnet gen_trt_engine.\n",
      "[06/27/2023-20:49:08] [TRT] [I] [MemUsageChange] Init CUDA: CPU +318, GPU +0, now: CPU 357, GPU 719 (MiB)\n",
      "[06/27/2023-20:49:10] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +443, GPU +116, now: CPU 855, GPU 835 (MiB)\n",
      "[06/27/2023-20:49:10] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "Parsing ONNX model\n",
      "[06/27/2023-20:49:10] [TRT] [W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[06/27/2023-20:49:10] [TRT] [I] No importer registered for op: ModulatedDeformConv2d. Attempting to import as plugin.\n",
      "[06/27/2023-20:49:10] [TRT] [I] Searching for plugin: ModulatedDeformConv2d, plugin_version: 1, plugin_namespace: \n",
      "[06/27/2023-20:49:10] [TRT] [I] Successfully created plugin: ModulatedDeformConv2d\n",
      "[06/27/2023-20:49:10] [TRT] [I] No importer registered for op: ModulatedDeformConv2d. Attempting to import as plugin.\n",
      "[06/27/2023-20:49:10] [TRT] [I] Searching for plugin: ModulatedDeformConv2d, plugin_version: 1, plugin_namespace: \n",
      "[06/27/2023-20:49:10] [TRT] [I] Successfully created plugin: ModulatedDeformConv2d\n",
      "[06/27/2023-20:49:10] [TRT] [I] No importer registered for op: ModulatedDeformConv2d. Attempting to import as plugin.\n",
      "[06/27/2023-20:49:10] [TRT] [I] Searching for plugin: ModulatedDeformConv2d, plugin_version: 1, plugin_namespace: \n",
      "[06/27/2023-20:49:10] [TRT] [I] Successfully created plugin: ModulatedDeformConv2d\n",
      "[06/27/2023-20:49:10] [TRT] [I] No importer registered for op: ModulatedDeformConv2d. Attempting to import as plugin.\n",
      "[06/27/2023-20:49:10] [TRT] [I] Searching for plugin: ModulatedDeformConv2d, plugin_version: 1, plugin_namespace: \n",
      "[06/27/2023-20:49:10] [TRT] [I] Successfully created plugin: ModulatedDeformConv2d\n",
      "[06/27/2023-20:49:10] [TRT] [I] No importer registered for op: ModulatedDeformConv2d. Attempting to import as plugin.\n",
      "[06/27/2023-20:49:10] [TRT] [I] Searching for plugin: ModulatedDeformConv2d, plugin_version: 1, plugin_namespace: \n",
      "[06/27/2023-20:49:10] [TRT] [I] Successfully created plugin: ModulatedDeformConv2d\n",
      "[06/27/2023-20:49:10] [TRT] [I] No importer registered for op: ModulatedDeformConv2d. Attempting to import as plugin.\n",
      "[06/27/2023-20:49:10] [TRT] [I] Searching for plugin: ModulatedDeformConv2d, plugin_version: 1, plugin_namespace: \n",
      "[06/27/2023-20:49:10] [TRT] [I] Successfully created plugin: ModulatedDeformConv2d\n",
      "Network Description\n",
      "Input 'input' with shape (-1, 3, -1, -1) and dtype DataType.FLOAT\n",
      "Output 'pred' with shape (-1, 1, -1, -1) and dtype DataType.FLOAT\n",
      "dynamic batch size handling\n",
      "Calibrating using ImageBatcher\n",
      "[8, 3, 736, 1280]\n",
      "<class 'numpy.float32'>\n",
      "TensorRT engine build configurations:\n",
      " \n",
      "  BuilderFlag.FP16\n",
      "  BuilderFlag.INT8\n",
      "  BuilderFlag.TF32\n",
      " \n",
      "  Note: max representabile value is 2,147,483,648 bytes or 2GB.\n",
      "  MemoryPoolType.WORKSPACE = 1125899906842624000 bytes\n",
      "  MemoryPoolType.DLA_MANAGED_SRAM = 0 bytes\n",
      "  MemoryPoolType.DLA_LOCAL_DRAM = 1073741824 bytes\n",
      "  MemoryPoolType.DLA_GLOBAL_DRAM = 536870912 bytes\n",
      " \n",
      "  Tactic Sources = 31\n",
      "[06/27/2023-20:49:11] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +854, GPU +362, now: CPU 1759, GPU 1285 (MiB)\n",
      "[06/27/2023-20:49:11] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +126, GPU +58, now: CPU 1885, GPU 1343 (MiB)\n",
      "[06/27/2023-20:49:11] [TRT] [I] Timing cache disabled. Turning it on will improve builder speed.\n",
      "[06/27/2023-20:49:11] [TRT] [W] Calibration Profile is not defined. Calibrating with Profile 0\n",
      "[06/27/2023-20:49:12] [TRT] [I] Total Activation Memory: 1125899909981412352\n",
      "[06/27/2023-20:49:12] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[06/27/2023-20:49:12] [TRT] [I] Total Host Persistent Memory: 9424\n",
      "[06/27/2023-20:49:12] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[06/27/2023-20:49:12] [TRT] [I] Total Scratch Memory: 67829760\n",
      "[06/27/2023-20:49:12] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 518 MiB\n",
      "[06/27/2023-20:49:12] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 106 steps to complete.\n",
      "[06/27/2023-20:49:13] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 4.15676ms to assign 9 blocks to 106 nodes requiring 684447744 bytes.\n",
      "[06/27/2023-20:49:13] [TRT] [I] Total Activation Memory: 684447744\n",
      "[06/27/2023-20:49:13] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2448, GPU 1637 (MiB)\n",
      "[06/27/2023-20:49:13] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 2449, GPU 1647 (MiB)\n",
      "[06/27/2023-20:49:13] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2448, GPU 1623 (MiB)\n",
      "[06/27/2023-20:49:13] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2448, GPU 1631 (MiB)\n",
      "[06/27/2023-20:49:13] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +652, now: CPU 0, GPU 701 (MiB)\n",
      "[06/27/2023-20:49:13] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[06/27/2023-20:49:13] [TRT] [I] Starting Calibration.\n",
      "Calibrating image 8 / 16\n",
      "[06/27/2023-20:49:15] [TRT] [I]   Calibrated batch 0 in 1.07023 seconds.\n",
      "Calibrating image 16 / 16\n",
      "[06/27/2023-20:49:18] [TRT] [I]   Calibrated batch 1 in 1.0564 seconds.\n",
      "Finished calibration batches\n",
      "[06/27/2023-20:49:29] [TRT] [I]   Post Processing Calibration data in 11.0225 seconds.\n",
      "[06/27/2023-20:49:29] [TRT] [I] Calibration completed in 18.6997 seconds.\n",
      "[06/27/2023-20:49:29] [TRT] [I] Writing Calibration Cache for calibrator: TRT-8503-EntropyCalibration2\n",
      "[ERROR] Exception caught in write_calibration_cache(): FileNotFoundError: [Errno 2] No such file or directory: '/results/export/cal.bin'\n",
      "\n",
      "At:\n",
      "  <frozen engine.calibrator>(115): write_calibration_cache\n",
      "  <frozen engine.builder>(274): create_engine\n",
      "  /usr/local/lib/python3.8/dist-packages/nvidia_tao_deploy/cv/ocdnet/scripts/gen_trt_engine.py(85): main\n",
      "  <frozen cv.common.decorators>(35): _func\n",
      "  /usr/local/lib/python3.8/dist-packages/hydra/core/utils.py(186): run_job\n",
      "  /usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py(119): run\n",
      "  /usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py(453): <lambda>\n",
      "  /usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py(213): run_and_report\n",
      "  /usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py(452): _run_app\n",
      "  /usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py(389): _run_hydra\n",
      "  <frozen cv.common.hydra.hydra_runner>(86): wrapper\n",
      "  /usr/local/lib/python3.8/dist-packages/nvidia_tao_deploy/cv/ocdnet/scripts/gen_trt_engine.py(98): <module>\n",
      "\n",
      "[06/27/2023-20:49:29] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +10, now: CPU 2510, GPU 1581 (MiB)\n",
      "[06/27/2023-20:49:29] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2510, GPU 1589 (MiB)\n",
      "[06/27/2023-20:49:29] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/27/2023-20:56:09] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[06/27/2023-20:56:09] [TRT] [I] Total Activation Memory: 1125899907635743232\n",
      "[06/27/2023-20:56:09] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[06/27/2023-20:56:09] [TRT] [I] Total Host Persistent Memory: 95232\n",
      "[06/27/2023-20:56:09] [TRT] [I] Total Device Persistent Memory: 2832896\n",
      "[06/27/2023-20:56:09] [TRT] [I] Total Scratch Memory: 135659520\n",
      "[06/27/2023-20:56:09] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 215 MiB, GPU 8709 MiB\n",
      "[06/27/2023-20:56:09] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 101 steps to complete.\n",
      "[06/27/2023-20:56:09] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 5.55043ms to assign 9 blocks to 101 nodes requiring 267179520 bytes.\n",
      "[06/27/2023-20:56:09] [TRT] [I] Total Activation Memory: 267179520\n",
      "[06/27/2023-20:56:09] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 2613, GPU 3043 (MiB)\n",
      "[06/27/2023-20:56:09] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2613, GPU 3051 (MiB)\n",
      "[06/27/2023-20:56:09] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[06/27/2023-20:56:09] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[06/27/2023-20:56:09] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[06/27/2023-20:56:09] [TRT] [W] - 29 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[06/27/2023-20:56:09] [TRT] [W] - 6 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[06/27/2023-20:56:09] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +6, GPU +32, now: CPU 6, GPU 32 (MiB)\n",
      "Generate TensorRT engine and calibration cache file successfully.\n",
      "Gen_trt_engine finished successfully.\n",
      "Sending telemetry data.\n",
      "Telemetry data couldn't be sent, but the command ran successfully.\n",
      "[WARNING]: <urlopen error [Errno -2] Name or service not known>\n",
      "Execution status: PASS\n"
     ]
    }
   ],
   "source": [
    "#FYI only if using direct container calls, do not both the previous and this block, just pick one option \n",
    "# Generate TensorRT engine using tao-deploy, direct container call\n",
    "!docker run -it --rm --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/pdelafuente/tao:/workspace/tao --gpus all nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-deploy \\\n",
    "ocdnet gen_trt_engine -e $SPECS_DIR/gen_trt_engine.yaml \\\n",
    "gen_trt_engine.onnx_file=$RESULTS_DIR/export/model_best.onnx \\\n",
    "                               gen_trt_engine.tensorrt.min_batch_size=1 \\\n",
    "                               gen_trt_engine.tensorrt.opt_batch_size=3 \\\n",
    "                               gen_trt_engine.tensorrt.max_batch_size=3 \\\n",
    "                               gen_trt_engine.tensorrt.data_type=int8 \\\n",
    "                               gen_trt_engine.trt_engine=$RESULTS_DIR/export/ocdnet_model.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea72101-e581-4edb-8db6-509d299524af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with generated TensorRT engine\n",
    "%env CUDA_MODULE_LOADING=\"LAZY\"\n",
    "!tao deploy ocdnet evaluate -e $SPECS_DIR/evaluate.yaml \\\n",
    "                             evaluate.trt_engine=$RESULTS_DIR/export/ocdnet_model.engine\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a1b1f3-e6df-4cd4-abe3-a93e32fe68a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_MODULE_LOADING=\"LAZY\"\n",
      "\n",
      "===========================\n",
      "=== TAO Toolkit PyTorch ===\n",
      "===========================\n",
      "\n",
      "NVIDIA Release 4.0.0-PyTorch (build 53420872)\n",
      "TAO Toolkit Version 4.0.0\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the TAO Toolkit End User License Agreement.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/tao-toolkit-software-license-agreement\n",
      "\n",
      "[2023-06-27 21:07:28,694 - TAO Toolkit - torch.distributed.nn.jit.instantiator - INFO] Created a temporary directory at /tmp/tmpkc_smjc6\n",
      "[2023-06-27 21:07:28,694 - TAO Toolkit - torch.distributed.nn.jit.instantiator - INFO] Writing /tmp/tmpkc_smjc6/_remote_module_non_scriptable.py\n",
      "[2023-06-27 21:07:29,152 - TAO Toolkit - matplotlib.font_manager - INFO] generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'evaluate.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "<frozen core.hydra.hydra_runner>:107: UserWarning: \n",
      "'evaluate.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "Experiment configuration:\n",
      "train:\n",
      "  results_dir: null\n",
      "  resume_training_checkpoint_path: null\n",
      "  num_epochs: 50\n",
      "  checkpoint_interval: 1\n",
      "  validation_interval: 1\n",
      "  gpu_id:\n",
      "  - 0\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: ???\n",
      "      box_thresh: ???\n",
      "      max_candidates: ???\n",
      "      unclip_ratio: ???\n",
      "  metric:\n",
      "    type: QuadMetric\n",
      "    args:\n",
      "      is_output_polygon: ???\n",
      "  trainer:\n",
      "    is_output_polygon: false\n",
      "    warmup_epoch: 3\n",
      "    seed: 2\n",
      "    log_iter: 10\n",
      "    clip_grad_norm: 5.0\n",
      "    show_images_iter: 50\n",
      "    tensorboard: false\n",
      "  loss:\n",
      "    type: DBLoss\n",
      "    alpha: 5\n",
      "    beta: 10\n",
      "    ohem_ratio: 3\n",
      "  optimizer:\n",
      "    type: Adam\n",
      "    args:\n",
      "      lr: 0.001\n",
      "      weight_decay: 0.0\n",
      "      amsgrad: true\n",
      "  lr_scheduler:\n",
      "    type: WarmupPolyLR\n",
      "    args:\n",
      "      warmup_epoch: ???\n",
      "model:\n",
      "  backbone: deformable_resnet18\n",
      "  pretrained: false\n",
      "  in_channels: 3\n",
      "  neck: FPN\n",
      "  inner_channels: 256\n",
      "  head: DBHead\n",
      "  out_channels: 2\n",
      "  k: 50\n",
      "  load_pruned_graph: false\n",
      "  pruned_graph_path: /results/prune/pruned_0.1.pth\n",
      "  pretrained_model_path: null\n",
      "evaluate:\n",
      "  results_dir: /results/evaluate\n",
      "  checkpoint: /workspace/tao/my_apps/ocdnet/results/train/model_best.pth\n",
      "  trt_engine: /workspace/tao/my_apps/ocdnet/results/export/ocdnet_model.engine\n",
      "  gpu_id: 0\n",
      "  batch_size: 1\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: 0.3\n",
      "      box_thresh: 0.55\n",
      "      max_candidates: 1000\n",
      "      unclip_ratio: 1.5\n",
      "  metric:\n",
      "    type: QuadMetric\n",
      "    args:\n",
      "      is_output_polygon: false\n",
      "dataset:\n",
      "  train_dataset:\n",
      "    data_name: ICDAR2015Dataset\n",
      "    data_path: ???\n",
      "    args:\n",
      "      img_mode: BGR\n",
      "      filter_keys:\n",
      "      - img_path\n",
      "      - img_name\n",
      "      - text_polys\n",
      "      - texts\n",
      "      - ignore_tags\n",
      "      - shape\n",
      "      ignore_tags:\n",
      "      - '*'\n",
      "      - '###'\n",
      "      pre_processes: null\n",
      "    loader:\n",
      "      batch_size: 32\n",
      "      shuffle: true\n",
      "      pin_memory: false\n",
      "      num_workers: 0\n",
      "      collate_fn: ''\n",
      "  validate_dataset:\n",
      "    data_name: ICDAR2015Dataset\n",
      "    data_path:\n",
      "    - /workspace/tao/my_apps/ocdnet/data/ocdnet/test\n",
      "    args:\n",
      "      img_mode: BGR\n",
      "      filter_keys: []\n",
      "      ignore_tags:\n",
      "      - '*'\n",
      "      - '###'\n",
      "      pre_processes:\n",
      "      - type: Resize2D\n",
      "        args:\n",
      "          short_size:\n",
      "          - 2464\n",
      "          - 3520\n",
      "          resize_text_polys: true\n",
      "    loader:\n",
      "      batch_size: 1\n",
      "      shuffle: false\n",
      "      pin_memory: false\n",
      "      num_workers: 4\n",
      "      collate_fn: ICDARCollateFN\n",
      "export:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  onnx_file: null\n",
      "  gpu_id: 0\n",
      "  width: ???\n",
      "  height: ???\n",
      "  opset_version: 11\n",
      "gen_trt_engine:\n",
      "  results_dir: null\n",
      "  gpu_id: 0\n",
      "  onnx_file: ???\n",
      "  trt_engine: ???\n",
      "  width: ???\n",
      "  height: ???\n",
      "  img_mode: BGR\n",
      "  tensorrt:\n",
      "    data_type: FP32\n",
      "    workspace_size: 1024\n",
      "    min_batch_size: 1\n",
      "    opt_batch_size: 1\n",
      "    max_batch_size: 1\n",
      "    calibration:\n",
      "      cal_image_dir: ???\n",
      "      cal_cache_file: ???\n",
      "      cal_batch_size: 1\n",
      "      cal_num_batches: 1\n",
      "inference:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  trt_engine: null\n",
      "  input_folder: ???\n",
      "  width: ???\n",
      "  height: ???\n",
      "  img_mode: ???\n",
      "  polygon: true\n",
      "  show: false\n",
      "  gpu_id: 0\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: ???\n",
      "      box_thresh: ???\n",
      "      max_candidates: ???\n",
      "      unclip_ratio: ???\n",
      "prune:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  gpu_id: 0\n",
      "  batch_size: 1\n",
      "  pruning_thresh: ???\n",
      "name: ???\n",
      "num_gpus: 1\n",
      "results_dir: ???\n",
      "\n",
      "Starting OCDNet evaluation\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "test model: 100%|█████████████████████████████| 310/310 [03:40<00:00,  1.41it/s]\n",
      "FPS:6.2781078920391655\n",
      "Precision:  0.9573949541905491\n",
      "Recall:  0.890591544565263\n",
      "Hmean:  0.9227857962941622\n",
      "Evaluation finished successfully.\n"
     ]
    }
   ],
   "source": [
    "#FYI only if using direct container calls, do not both the previous and this block, just pick one option \n",
    "# Evaluate with generated TensorRT engine - direct docker call\n",
    "%env CUDA_MODULE_LOADING=\"LAZY\"\n",
    "!docker run -it --rm --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/pdelafuente/tao:/workspace/tao --gpus all nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-pyt \\\n",
    "ocdnet evaluate -e $SPECS_DIR/evaluate.yaml \\\n",
    "                             evaluate.trt_engine=$RESULTS_DIR/export/ocdnet_model.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232df6d-e764-441b-ab2f-a2ceaa54ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with generated TensorRT engine\n",
    "!tao deploy ocdnet inference -e $SPECS_DIR/inference.yaml \\\n",
    "                              inference.trt_engine=$RESULTS_DIR/export/ocdnet_model.engine \\\n",
    "                              inference.input_folder=$DATA_DIR/test/img \\\n",
    "                              inference.results_dir=$RESULTS_DIR/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f730908b-8a02-4ca6-8b16-e944b164aa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "=== TAO Toolkit PyTorch ===\n",
      "===========================\n",
      "\n",
      "NVIDIA Release 4.0.0-PyTorch (build 53420872)\n",
      "TAO Toolkit Version 4.0.0\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the TAO Toolkit End User License Agreement.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/tao-toolkit-software-license-agreement\n",
      "\n",
      "[2023-06-27 21:15:30,624 - TAO Toolkit - torch.distributed.nn.jit.instantiator - INFO] Created a temporary directory at /tmp/tmp1i1qpz0r\n",
      "[2023-06-27 21:15:30,625 - TAO Toolkit - torch.distributed.nn.jit.instantiator - INFO] Writing /tmp/tmp1i1qpz0r/_remote_module_non_scriptable.py\n",
      "[2023-06-27 21:15:31,084 - TAO Toolkit - matplotlib.font_manager - INFO] generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'inference.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "<frozen core.hydra.hydra_runner>:107: UserWarning: \n",
      "'inference.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "Experiment configuration:\n",
      "train:\n",
      "  results_dir: null\n",
      "  resume_training_checkpoint_path: null\n",
      "  num_epochs: 50\n",
      "  checkpoint_interval: 1\n",
      "  validation_interval: 1\n",
      "  gpu_id:\n",
      "  - 0\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: ???\n",
      "      box_thresh: ???\n",
      "      max_candidates: ???\n",
      "      unclip_ratio: ???\n",
      "  metric:\n",
      "    type: QuadMetric\n",
      "    args:\n",
      "      is_output_polygon: ???\n",
      "  trainer:\n",
      "    is_output_polygon: false\n",
      "    warmup_epoch: 3\n",
      "    seed: 2\n",
      "    log_iter: 10\n",
      "    clip_grad_norm: 5.0\n",
      "    show_images_iter: 50\n",
      "    tensorboard: false\n",
      "  loss:\n",
      "    type: DBLoss\n",
      "    alpha: 5\n",
      "    beta: 10\n",
      "    ohem_ratio: 3\n",
      "  optimizer:\n",
      "    type: Adam\n",
      "    args:\n",
      "      lr: 0.001\n",
      "      weight_decay: 0.0\n",
      "      amsgrad: true\n",
      "  lr_scheduler:\n",
      "    type: WarmupPolyLR\n",
      "    args:\n",
      "      warmup_epoch: ???\n",
      "model:\n",
      "  backbone: deformable_resnet18\n",
      "  pretrained: false\n",
      "  in_channels: 3\n",
      "  neck: FPN\n",
      "  inner_channels: 256\n",
      "  head: DBHead\n",
      "  out_channels: 2\n",
      "  k: 50\n",
      "  load_pruned_graph: false\n",
      "  pruned_graph_path: /results/prune/pruned_0.1.pth\n",
      "  pretrained_model_path: null\n",
      "evaluate:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  trt_engine: null\n",
      "  gpu_id: 0\n",
      "  batch_size: 1\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: ???\n",
      "      box_thresh: ???\n",
      "      max_candidates: ???\n",
      "      unclip_ratio: ???\n",
      "  metric:\n",
      "    type: QuadMetric\n",
      "    args:\n",
      "      is_output_polygon: ???\n",
      "dataset:\n",
      "  train_dataset:\n",
      "    data_name: ICDAR2015Dataset\n",
      "    data_path: ???\n",
      "    args:\n",
      "      img_mode: BGR\n",
      "      filter_keys:\n",
      "      - img_path\n",
      "      - img_name\n",
      "      - text_polys\n",
      "      - texts\n",
      "      - ignore_tags\n",
      "      - shape\n",
      "      ignore_tags:\n",
      "      - '*'\n",
      "      - '###'\n",
      "      pre_processes: null\n",
      "    loader:\n",
      "      batch_size: 32\n",
      "      shuffle: true\n",
      "      pin_memory: false\n",
      "      num_workers: 0\n",
      "      collate_fn: ''\n",
      "  validate_dataset:\n",
      "    data_name: ICDAR2015Dataset\n",
      "    data_path: ???\n",
      "    args:\n",
      "      img_mode: BGR\n",
      "      filter_keys:\n",
      "      - ''\n",
      "      ignore_tags:\n",
      "      - '*'\n",
      "      - '###'\n",
      "      pre_processes: null\n",
      "    loader:\n",
      "      batch_size: 1\n",
      "      shuffle: false\n",
      "      pin_memory: false\n",
      "      num_workers: 0\n",
      "      collate_fn: ICDARCollateFN\n",
      "export:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  onnx_file: null\n",
      "  gpu_id: 0\n",
      "  width: ???\n",
      "  height: ???\n",
      "  opset_version: 11\n",
      "gen_trt_engine:\n",
      "  results_dir: null\n",
      "  gpu_id: 0\n",
      "  onnx_file: ???\n",
      "  trt_engine: ???\n",
      "  width: ???\n",
      "  height: ???\n",
      "  img_mode: BGR\n",
      "  tensorrt:\n",
      "    data_type: FP32\n",
      "    workspace_size: 1024\n",
      "    min_batch_size: 1\n",
      "    opt_batch_size: 1\n",
      "    max_batch_size: 1\n",
      "    calibration:\n",
      "      cal_image_dir: ???\n",
      "      cal_cache_file: ???\n",
      "      cal_batch_size: 1\n",
      "      cal_num_batches: 1\n",
      "inference:\n",
      "  results_dir: /workspace/tao/my_apps/ocdnet/inference\n",
      "  checkpoint: /workspace/tao/my_apps/ocdnet/results/train/model_best.pth\n",
      "  trt_engine: /workspace/tao/my_apps/ocdnet/export/ocdnet_model.engine\n",
      "  input_folder: /workspace/tao/my_apps/ocdnet/data/ocdnet/test/img\n",
      "  width: 1984\n",
      "  height: 1984\n",
      "  img_mode: BGR\n",
      "  polygon: false\n",
      "  show: false\n",
      "  gpu_id: 0\n",
      "  post_processing:\n",
      "    type: SegDetectorRepresenter\n",
      "    args:\n",
      "      thresh: 0.1\n",
      "      box_thresh: 0.3\n",
      "      max_candidates: 1000\n",
      "      unclip_ratio: 1.5\n",
      "prune:\n",
      "  results_dir: null\n",
      "  checkpoint: ???\n",
      "  gpu_id: 0\n",
      "  batch_size: 1\n",
      "  pruning_thresh: ???\n",
      "name: ???\n",
      "num_gpus: 1\n",
      "results_dir: ???\n",
      "\n",
      "<frozen core.loggers.api_logging>:195: UserWarning: Log file already exists at /workspace/tao/my_apps/ocdnet/inference/status.json\n",
      "Starting OCDNet inference\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "100%|█████████████████████████████████████████| 310/310 [02:37<00:00,  1.97it/s]\n",
      "Inference finished successfully.\n"
     ]
    }
   ],
   "source": [
    "#FYI only if using direct container calls, do not both the previous and this block, just pick one option \n",
    "# Inference with generated TensorRT engine - direct container call\n",
    "!docker run -it --rm --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/pdelafuente/tao:/workspace/tao --gpus all nvcr.io/ea-tlt/tao_ea/tao-toolkit:5.0.0-ea-pyt \\\n",
    "ocdnet inference -e $SPECS_DIR/inference.yaml \\\n",
    "inference.trt_engine=$RESULTS_DIR/export/ocdnet_model.engine \\\n",
    "                              inference.input_folder=$DATA_DIR/test/img \\\n",
    "                              inference.results_dir=$RESULTS_DIR/inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c656b2-b811-45b4-80d4-71e7f4d92fb8",
   "metadata": {},
   "source": [
    "This completes the OCDNet portion.  Proceed to the OCRNet notebook to learn how to train the OCR model for handwritting detection.\n",
    "\n",
    "The combination of both the OCDNet and OCRnet models are used together for identify areas on the documents that have handwritten content and then identifying what is handwritten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e37dda-9ec5-4b92-afba-0ed686e6acba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
